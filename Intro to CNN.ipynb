{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to convolutional neural networks?\n",
    "\n",
    "### 1.1 Why?\n",
    "\n",
    "#### 1.1.1 CNNs can deal better with large images (until now, images used were fairly small)\n",
    "\n",
    "- Imagine an color image with 500 x 500 pixels, this means you would    end up having 500 x 500 x 3 = 750,000 input features, $(x_1,...,x_{750,000})$.\n",
    "- next, imagine having 2000 hidden units in the first hidden layer. Then the matrix $w^{[1]}$ would have dimensions (2000 x 750,000), and will have 1.5 billion parameters. So it becomes a very high-dimensional problem!\n",
    "\n",
    "#### 1.1.1 CNNs have certain features that identify patterns in images because of  \"convolution operation\"\n",
    "\n",
    "- Dense layers learn global patterns in their input feature space\n",
    "\n",
    "- Convolution layers learn local patterns, and this leads to the following interesting features:\n",
    "    - Unlike with densely connected networks, when a convolutional neural network recognizes a patterns let's say, in the upper-right corner of a picture, it can recognize it anywhere else in a picture. \n",
    "    - Deeper convolutional neural networks can learn spatial hierarchies. A first layer will learn small local patterns, a second layer will learn larger patterns using features of the first layer patterns, etc. \n",
    "     \n",
    "\n",
    "### 1.2 What are they used for?\n",
    "- Image classification\n",
    "- Object detection in images\n",
    "- Picture neural style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The convolution operation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The basic convolution operation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea: detect edges in your image. Typically, we'll detect vertical or horizontal edges. Let's look at what horizontal edge detection would look like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simplified 5 x 5 pixel image (greyscale!). You use a so-called \"filter\" (denoted on the right) to perform a convolution operation. This particular filter operation will detect horizontal edges. The matrix in the left should have number in it (from 1-255, or let's assume we rescaled it to number 1-10). The output is a 3 x 3 matrix. (*This example is for computational clarity, no clear edges*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What dimension would the output matrix have had if we had started from a 7 x 7 matrix? And a 64 x 64 matrix?\n",
    "\n",
    "(*Then, create a new example with a clear edge and look at the output*)\n",
    "\n",
    "In Keras, function for the convolution step is `Conv2D`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "downsides of using filters in images:\n",
    "- image shrinks with each convolution layer and you're throwing away information in each layer!\n",
    "    - Starting from a 5 x 5 matrix, and using a 3 x 3 matrix, you end up with a 3 x 3 image. \n",
    "    - Starting from a 10 x 10 matrix, and using a 3 x 3 matrix, you end up with a 8 x 8 image. \n",
    "    - etc.\n",
    "- pixels around the edges are used much less in the outputs because of the way that filters work.\n",
    "\n",
    "Solution for both of these problems: pad your image before applying the convolution! just one layer of pixels around the edges preserves the image size when having a 3 x 3 filter. We can also use bigger filters, but generally the dimensions are odd!\n",
    "\n",
    "Terminology:\n",
    "\n",
    "- \"Valid\" - no padding\n",
    "- \"Same\" - padding such that output is same as the input size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Strided convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step over 2 steps instead of 1 (\"stride\" = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Convolutions on RGB images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of 5 x 5 grayscale, look at a 7 x 7 RGB image, which boils down to having a 7 x 7 x 3 tensor. Then, you need to use a filter that has the third dimension equal to 3 as well, let's say, 3 x 3 x 3 (a 3D \"cube\"). the output is a 5 x 5 image. So, how does this work? multiply all the 27 number in the cube for each stride.\n",
    "\n",
    "This allows us to detect, eg only horizontal edges in the blue channel (filter on the red and green channel all equal to 0). \n",
    "\n",
    "Then, in each layer, you can convolve with several 3D filters.\n",
    "Then, you stack every output result together, and that way you end up having a 5 x 5 x (number of filters) shape.\n",
    "\n",
    "\n",
    "If you think of it, the filter plays the same role as the w^{[1]} in our densely connected networks.\n",
    "\n",
    "The advantage is, your image can be huge, the amount of parameters you have still only depends on how many filters you're using!\n",
    "\n",
    "\n",
    "Imagine 20 (3 x 3 x 3) --> 20 * 27 + a bias for each filter (1* 20) = 560 parameters.\n",
    "\n",
    "Notation:\n",
    "\n",
    "- $f^{[l]}$ = size of the filter\n",
    "- $p^{[l]}$ = padding\n",
    "- $s^{[l]}$ = amount of stride\n",
    "- $ n_c^{[l]}$ = number of filters\n",
    "- filter: $f^{[l]}$ x $f^{[l]}$ x $ n_c^{[l-1]}$\n",
    "\n",
    "\n",
    "- Input = $ n_h^{[l-1]} * n_w^{[l-1]} * n_c^{[l-1]} $\n",
    "- Output = $ n_h^{[l]} * n_w^{[l]} * n_c^{[l]} $\n",
    "\n",
    "Height and width are given by:\n",
    "\n",
    "$n_h^{[l]}= \\Bigr\\lfloor\\dfrac{n_h^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1\\Bigr\\rfloor$\n",
    "\n",
    "$n_w^{[l]}= \\Bigr\\lfloor\\dfrac{n_w^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1\\Bigr\\rfloor$\n",
    "\n",
    "\n",
    "Activations: $a^{[l]}$ is of dimension $ n_h^{[l]} * n_w^{[l]} * n_c^{[l]} $\n",
    "\n",
    "**example: walk through a network genre https://www.coursera.org/learn/convolutional-neural-networks/lecture/A9lXL/simple-convolutional-network-example, or (after pooling) https://www.coursera.org/learn/convolutional-neural-networks/lecture/uRYL1/cnn-example**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling example break \n",
    "Hyperparameters:\n",
    "- f (filter size)\n",
    "- S (stride)\n",
    "\n",
    "Common: f=2, s=2 and f=3, s=2, this shrinks the size of the representations.\n",
    "If a feature is detected anywhere in the quadrants, a high number will appear. so max pooling preserves this feature.\n",
    "\n",
    "Except for hyperparameters, no parameters in max pooling layers!\n",
    "\n",
    "Max pooling on a 3d input: just do the same for several channels.\n",
    "\n",
    "another way of pooling = *average pooling*\n",
    "\n",
    "We'll treat Convolutional layers plus pooling layers as 1 layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. fully connected layers in your CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add fully connected layers towards the end of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n",
    "\n",
    "https://datascience.stackexchange.com/questions/16463/what-is-are-the-default-filters-used-by-keras-convolution2d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
